#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from workalendar.asia import China
import chinese_calendar as calendar
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LassoCV
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from sklearn.metrics import make_scorer, recall_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import TimeSeriesSplit
from gplearn.genetic import SymbolicTransformer
from xgboost import XGBRegressor
from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import PolynomialFeatures


# In[2]:


df_raw = pd.read_csv('./ncd5.csv',header=None)
df = df_raw.iloc[4:, :].copy()
df.columns = [
    "date",
    "reverse_repo_7d",
    "reverse_repo_7d_amount",
    "MLF_1y",
    "DXY",
    "DR001",
    "DR007",
    "FR007",
    "CPI_yoy",
    "CPI_mom",
    'CPI',
    "PPI_yoy",
    "PPI_mom",
    "house_listing_index",
    "PMI",
    "tsf",
    "trust_loan",
    "m1_yoy",
    "m2_yoy",
    "tsf_yoy",
    "Shibor",
    "central_bank_bill",
    "ncd_amount",
    "interbank",
    'IBO001',
    'IBO007',
    'SHCOMP',
    'R001',
    'R007',
    "ncd",
    'ncd3m',
    'ncd1y'
]

# åœ¨åŒä¸šå­˜å•ä¹‹åå‡ºç°çš„æ•°æ®ï¼ˆä¸ºäº†ç¡®ä¿æˆ‘ä»¬æœ‰å°½å¯èƒ½å¤šçš„æ•°æ®ï¼Œåœ¨è¿™é‡Œåˆ é™¤ï¼ŒåŒæ—¶è¿™å‡ ä¸ªæŒ‡æ ‡å’Œå…¶ä»–ä¸€äº›æŒ‡æ ‡å…³è”æ€§å¤§ï¼Œæ‰€ä»¥åˆ å‡ä¹‹åä¸ä¼šæœ‰å¤ªå¤§é—®é¢˜ï¼‰
cols_to_drop = [col for col in df.columns if any(key in col for key in ['DR007', 'MLF_1y', 'DR001'])]
# cols_to_drop = [col for col in df.columns if any(key in col for key in ['MLF_1y'])]
df = df.drop(columns=cols_to_drop)

# æ¸…æ´—æ•°å­—åˆ—ï¼šå»æ‰é€—å·ã€ç™¾åˆ†å·ã€ä¸­æ–‡å•ä½ç­‰éæ•°å­—å­—ç¬¦
for col in df.columns:
    if col != "date":
        df[col] = (
            df[col]
            .astype(str)
            .str.replace(",", "", regex=False)   # å»é™¤åƒä½é€—å·
            .str.replace("%", "", regex=False)   # å»é™¤ç™¾åˆ†å·
            .str.replace("äº¿", "", regex=False)  # å»é™¤â€œäº¿â€å­—
        )
        df[col] = pd.to_numeric(df[col], errors="coerce")

# æŒ‰æ—¥æœŸå‡åºæ’åº
df = df.sort_values(by='date', ascending=True).reset_index(drop=True)

# è½¬æ¢ä¸ºdatetime
df["date"] = pd.to_datetime(df["date"])
# df = df[df["date"] >= pd.to_datetime("2019-01-01")].reset_index(drop=True)


# In[3]:


df.head()


# In[4]:


import matplotlib.dates as mdates
import matplotlib.pyplot as plt

# è®¾ç½®æ¯å¹´6æœˆå’Œ12æœˆæ˜¾ç¤ºä¸€æ¬¡
locator = mdates.MonthLocator(bymonth=[6, 12])
formatter = mdates.DateFormatter('%Y-%m')

for col in df.columns:
    plt.figure(figsize=(10, 4))
    plt.plot(df['date'], df[col])
    plt.title(f"{col} over time")
    plt.xlabel("Date")
    plt.ylabel(col)

    ax = plt.gca()
    ax.xaxis.set_major_locator(locator)
    ax.xaxis.set_major_formatter(formatter)

    plt.xticks(rotation=45)
    plt.grid(True)
    plt.tight_layout()
    plt.show()


# In[5]:


# åŠ æƒå¹³å‡
# æ‰¾å‡º CPI å€¼å˜åŒ–çš„â€œé”šç‚¹æ—¥æœŸâ€
cpi_series = df[["date", "CPI"]].copy()
cpi_series["prev_CPI"] = cpi_series["CPI"].shift(1)
cpi_series["changed"] = cpi_series["CPI"] != cpi_series["prev_CPI"]
release_dates = cpi_series[cpi_series["changed"]]["date"].reset_index(drop=True)

# ç¡®ä¿æŒ‰æ—¶é—´æ’åº
release_dates = release_dates.sort_values().reset_index(drop=True)

# åˆå§‹åŒ–æ–°çš„ CPI åˆ—
cpi_weighted = pd.Series(index=df.index, dtype=float)

# éå†ä¸¤ä¸ª CPI å‘å¸ƒæ—¥ä¹‹é—´çš„åŒºé—´ï¼Œåšçº¿æ€§æ’å€¼
for i in range(len(release_dates) - 1):
    start_date = release_dates[i]
    end_date = release_dates[i + 1]

    mask = (df["date"] >= start_date) & (df["date"] <= end_date)
    days_total = (end_date - start_date).days

    val_start = df.loc[df["date"] == start_date, "CPI"].values[0]
    val_end = df.loc[df["date"] == end_date, "CPI"].values[0]

    weights = (df.loc[mask, "date"] - start_date).dt.days / days_total
    interpolated = (1 - weights) * val_start + weights * val_end

    cpi_weighted.loc[mask] = interpolated

# è¾¹ç•Œå¤„ç†ï¼ˆæœ€å‰é¢ä¿æŒåŸå§‹å€¼æˆ–ç”¨ç¬¬ä¸€ä¸ªé”šç‚¹å€¼å¡«å……ï¼‰
first_value = df.loc[df["date"] == release_dates[0], "CPI"].values[0]
cpi_weighted[df["date"] < release_dates[0]] = first_value

# æœ€åä¸€ä¸ªåŒºé—´åé¢ï¼ˆå¯é€‰æ‹©æ˜¯å¦å¤–æ¨ï¼‰
last_value = df.loc[df["date"] == release_dates.iloc[-1], "CPI"].values[0]
cpi_weighted[df["date"] > release_dates.iloc[-1]] = last_value

# å°†æ—¥æœŸå’ŒåŠ æƒ CPI åˆå¹¶æˆä¸€ä¸ªè¡¨æ ¼æŸ¥çœ‹
cpi_result = pd.DataFrame({
    "date": df["date"],
    "cpi_weighted": cpi_weighted
})

# æ˜¾ç¤ºå‰å‡ è¡Œ
print(cpi_result.head(20))

# æˆ–æŸ¥çœ‹æ”¶å°¾æ•°æ®
print(cpi_result.tail(105))

# åŠ å…¥æ–°åˆ— CPI
df["CPI_weighted"] = cpi_weighted


# In[6]:


def smooth_indicator(df, col_name):
    """
    å¯¹æŸåˆ—è¿›è¡Œé”šç‚¹è¯†åˆ« + çº¿æ€§æ’å€¼å¹³æ»‘ï¼Œè¿”å›æ–°çš„ Seriesã€‚
    """
    series = df[["date", col_name]].copy()
    series["prev_val"] = series[col_name].shift(1)
    series["changed"] = series[col_name] != series["prev_val"]
    release_dates = series[series["changed"]]["date"].reset_index(drop=True).sort_values()

    # åˆå§‹åŒ–ç»“æœ
    smoothed = pd.Series(index=df.index, dtype=float)

    for i in range(len(release_dates) - 1):
        start_date = release_dates.iloc[i]
        end_date = release_dates.iloc[i + 1]
        mask = (df["date"] >= start_date) & (df["date"] <= end_date)
        days_total = (end_date - start_date).days

        val_start = df.loc[df["date"] == start_date, col_name].values[0]
        val_end = df.loc[df["date"] == end_date, col_name].values[0]
        weights = (df.loc[mask, "date"] - start_date).dt.days / days_total
        interpolated = (1 - weights) * val_start + weights * val_end

        smoothed.loc[mask] = interpolated

    # è¾¹ç•Œå¡«å……
    first_value = df.loc[df["date"] == release_dates.iloc[0], col_name].values[0]
    smoothed[df["date"] < release_dates.iloc[0]] = first_value

    last_value = df.loc[df["date"] == release_dates.iloc[-1], col_name].values[0]
    smoothed[df["date"] > release_dates.iloc[-1]] = last_value

    return smoothed


# In[7]:


cols_to_smooth = ["CPI_yoy", "CPI_mom", "PPI_yoy", "PPI_mom", "PMI", 
                  "house_listing_index"]

for col in cols_to_smooth:
    df[f"{col}"] = smooth_indicator(df, col)


# In[8]:


import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))

# å¯¹æ¯”ç»˜å›¾
plt.plot(df["date"], df["CPI"], label="Original CPI (Forward Filled)", color="orange", linestyle="--")
plt.plot(df["date"], cpi_weighted, label="Weighted CPI (Interpolated)", color="blue")

# æ ‡è®° CPI å‘å¸ƒæ—¥
plt.scatter(release_dates, df.loc[df["date"].isin(release_dates), "CPI"], 
            color='red', label="CPI Release Dates", zorder=5)

plt.title("CPI Interpolation vs Original Forward Filled")
plt.xlabel("Date")
plt.ylabel("CPI")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


# In[9]:


df.head()


# In[10]:


# è®¡ç®—è·ç¦»æœˆæœ«çš„å¤©æ•°
df["days_to_month_end"] = (df["date"].dt.to_period("M").dt.end_time - df["date"]).dt.days

# ç¨æœŸ
tax_dates = [
    # 2014
    "2014-01-16", "2014-02-21", "2014-03-17", "2014-04-18", "2014-05-19", "2014-06-16",
    "2014-07-15", "2014-08-15", "2014-09-15", "2014-10-22", "2014-11-17", "2014-12-15",
    # 2015
    "2015-01-19", "2015-02-15", "2015-03-16", "2015-04-20", "2015-05-18", "2015-06-15",
    "2015-07-15", "2015-08-17", "2015-09-15", "2015-10-22", "2015-11-16", "2015-12-15",
    # 2016
    "2016-01-18", "2016-02-22", "2016-03-15", "2016-04-18", "2016-05-16", "2016-06-20",
    "2016-07-15", "2016-08-15", "2016-09-18", "2016-10-24", "2016-11-15", "2016-12-15",
    # 2017
    "2017-01-16", "2017-02-15", "2017-03-15", "2017-04-18", "2017-05-15", "2017-06-15",
    "2017-07-17", "2017-08-15", "2017-09-15", "2017-10-23", "2017-11-15", "2017-12-15",
    # 2018
    "2018-01-15", "2018-02-22", "2018-03-15", "2018-04-18", "2018-05-15", "2018-06-15",
    "2018-07-16", "2018-08-15", "2018-09-17", "2018-10-24", "2018-11-15", "2018-12-17",
    # 2019
    "2019-01-15", "2019-02-26", "2019-03-15", "2019-04-18", "2019-05-21", "2019-06-19",
    "2019-07-15", "2019-08-16", "2019-09-18", "2019-10-24", "2019-11-15", "2019-12-16",
    # 2020
    "2020-01-15", "2020-02-17", "2020-03-16", "2020-04-20", "2020-05-22", "2020-06-20",
    "2020-08-17", "2020-10-23", "2020-11-16",
    # 2021
    "2021-01-20", "2021-02-23", "2021-04-20", "2021-05-21", "2021-06-18", "2021-08-16", "2021-10-26", "2021-12-17",
    # 2022
    "2022-01-19", "2022-02-23", "2022-04-20", "2022-05-19", "2022-06-20", "2022-09-20", "2022-10-25", "2022-12-15",
    # 2023
    "2023-01-16", "2023-04-17", "2023-05-18", "2023-07-17", "2023-10-23",
    # 2024
    "2024-01-15", "2024-02-23", "2024-04-18", "2024-05-22", "2024-06-19", "2024-09-18", "2024-10-24", "2024-12-16",
    # 2025
    "2025-01-15", "2025-02-20", "2025-03-17", "2025-04-18", "2025-05-22", "2025-06-16",
    "2025-10-27", "2025-11-17"
]

# Step 3: è½¬æ¢ä¸ºæ—¥æœŸå¹¶åˆ›å»ºç¨æœŸé›†åˆ
tax_days = pd.to_datetime(tax_dates)
tax_day_set = set(tax_days)

# Step 4: æ·»åŠ  is_taxday åˆ—ï¼ˆç¨æœŸ=1ï¼Œéç¨æœŸ=0ï¼‰
df['is_taxday'] = df['date'].isin(tax_day_set).astype(int)

    
# æ˜¥èŠ‚
# æ¯å¹´æ˜¥èŠ‚ï¼ˆå†œå†æ­£æœˆåˆä¸€ï¼‰å¯¹åº”çš„å…¬å†æ—¥æœŸï¼ˆæ¥æºï¼šä¸‡å¹´å†ï¼‰
spring_festival_dates = {
    2014: "2014-01-31",
    2015: "2015-02-19",
    2016: "2016-02-08",
    2017: "2017-01-28",
    2018: "2018-02-16",
    2019: "2019-02-05",
    2020: "2020-01-25",
    2021: "2021-02-12",
    2022: "2022-02-01",
    2023: "2023-01-22",
    2024: "2024-02-10",
    2025: "2025-01-29",
}

# åˆå§‹åŒ–æ˜¥èŠ‚å‰æ ‡è®°åˆ—
df["pre_spring_festival_flag"] = 0

# éå†æ¯å¹´æ˜¥èŠ‚ï¼Œæ ‡è®°æ˜¥èŠ‚å‰14å¤©èŒƒå›´
for year, festival_date_str in spring_festival_dates.items():
    festival_date = pd.to_datetime(festival_date_str)
    pre_period_start = festival_date - pd.Timedelta(days=14)
    pre_period_end = festival_date - pd.Timedelta(days=1)
    
    mask = (df["date"] >= pre_period_start) & (df["date"] <= pre_period_end)
    df.loc[mask, "pre_spring_festival_flag"] = 1

# åˆå§‹åŒ–æ˜¥èŠ‚åæ ‡è®°åˆ—
df["post_spring_festival_flag"] = 0

# éå†æ¯å¹´æ˜¥èŠ‚ï¼Œæ ‡è®°èŠ‚å14å¤©
for year, festival_date_str in spring_festival_dates.items():
    festival_date = pd.to_datetime(festival_date_str)
    post_period_start = festival_date
    post_period_end = festival_date + pd.Timedelta(days=13)  # å…±14å¤©ï¼š0~13

    mask = (df["date"] >= post_period_start) & (df["date"] <= post_period_end)
    df.loc[mask, "post_spring_festival_flag"] = 1


# In[11]:


df['3m1ydiff'] = df['ncd3m'] - df['ncd']


# In[12]:


df.tail(50)


# In[13]:


df.iloc[3150:3200]


# In[14]:


df.isna().sum()


# In[15]:


# æ ¹æ®æ”¶ç›Šç‡è®¡ç®—å…¶ä»–å› å­
## cntn
df["ncd"] = pd.to_numeric(df["ncd"], errors="coerce")
# æ”¶ç›Šç‡å·®åˆ†
df["ncd_diff"] = df["ncd"].diff()
# cntn(30): æœ€è¿‘30å¤©æ”¶ç›Šç‡å˜åŒ–<0çš„å¤©æ•°å æ¯”
df["cntn_30"] = (
    df["ncd_diff"]
    .rolling(30)
    .apply(lambda x: (x < 0).mean(), raw=True)
)


# In[16]:


# åè½¬è¶‹åŠ¿å› å­
def compute_imax(series, window=20):
    """
    è®¡ç®— imaxï¼šå½“å‰æ—¥ä¸è¿‡å» window æ—¥å†…æœ€é«˜å€¼æ‰€åœ¨ä½ç½®çš„é—´éš”å¤©æ•°ã€‚
    è¶Šå°è¡¨ç¤ºåˆšåˆ›æ–°é«˜ï¼Œè¶Šå¤§è¡¨ç¤ºå·²è¿œç¦»é«˜ç‚¹ã€‚
    """
    imax_list = []
    for i in range(len(series)):
        if i < window:
            imax_list.append(np.nan)
        else:
            window_slice = series[i - window:i]
            if window_slice.isna().all():
                imax_list.append(np.nan)
            else:
                max_idx = window_slice.idxmax()
                # æ”¹è¿™é‡Œï¼šç”¨ä½ç½®å·®è®¡ç®—å¤©æ•°
                days_since_high = i - 1 - max_idx
                imax_list.append(days_since_high)
    return pd.Series(imax_list, index=series.index)

df["imax_20"] = compute_imax(df["ncd"], window=20)


# In[17]:


def compute_atr(series, window=14):
    """
    ç”¨ ncd çš„ç»å¯¹å˜åŒ–è¿‘ä¼¼çœŸå®æ³¢å¹…
    """
    true_range = series.diff().abs()
    atr = true_range.rolling(window=window).mean()
    return atr

df["atr_14"] = compute_atr(df["ncd"], window=14)


# In[18]:


def compute_qtlu(series, window=30, quantile=0.8):
    """
    è®¡ç®—è¿‡å» window æ—¥çš„ quantile åˆ†ä½ç‚¹ï¼ˆå½“å‰ä¸º 80%ï¼‰
    """
    return series.rolling(window=window).quantile(quantile)

# æ„é€  qtlu: ncd çš„è¿‡å»30æ—¥ 80%åˆ†ä½ç‚¹
df["qtlu_30"] = compute_qtlu(df["ncd"], window=30, quantile=0.8)


# In[19]:


def compute_qtld(series, window=30, quantile=0.2):
    """
    è®¡ç®—è¿‡å» window æ—¥çš„ quantile åˆ†ä½ç‚¹ï¼ˆé»˜è®¤ 20%ï¼‰
    """
    return series.rolling(window=window).quantile(quantile)

# æ„é€  qtld: ncd çš„è¿‡å»30æ—¥ 20%åˆ†ä½ç‚¹
df["qtld_30"] = compute_qtld(df["ncd"], window=30, quantile=0.2)


# In[20]:


df.head(50)


# In[21]:


# å»é™¤æ‰€æœ‰å«NaNçš„è¡Œ
data = df.dropna().copy()
# é‡ç½®ç´¢å¼•
data.reset_index(drop=True, inplace=True)
data.head(50)


# In[22]:


data.isna().sum()


# In[23]:


import matplotlib.pyplot as plt
import seaborn as sns

# ç”»å›¾
plt.figure(figsize=(8, 5))
sns.histplot(data["ncd"], bins=30, kde=True, color="skyblue", edgecolor="black")

plt.title("NCD")
plt.xlabel("NCD")
plt.ylabel("count")
plt.grid(True)
plt.tight_layout()
plt.show()


# In[24]:


'''
import matplotlib.pyplot as plt
import seaborn as sns

# ç”»å›¾
plt.figure(figsize=(8, 5))
sns.histplot(data["log_ncd_next_week"], bins=30, kde=True, color="skyblue", edgecolor="black")

plt.title("NCD")
plt.xlabel("NCD")
plt.ylabel("count")
plt.grid(True)
plt.tight_layout()
plt.show()
'''


# In[25]:


data.columns


# In[26]:


# åˆ›å»ºä¸€ä¸ªæœˆä¹‹åçš„ncd
data['ncd_next_month'] = data["ncd"].shift(-30)
data = data.dropna().reset_index(drop=True)
data.head(35)


# In[27]:


import matplotlib.dates as mdates
import matplotlib.pyplot as plt

# è®¾ç½®æ¯å¹´6æœˆå’Œ12æœˆæ˜¾ç¤ºä¸€æ¬¡
locator = mdates.MonthLocator(bymonth=[6, 12])
formatter = mdates.DateFormatter('%Y-%m')

for col in data.columns:
    plt.figure(figsize=(10, 4))
    plt.plot(data['date'], data[col])
    plt.title(f"{col} over time")
    plt.xlabel("Date")
    plt.ylabel(col)

    ax = plt.gca()
    ax.xaxis.set_major_locator(locator)
    ax.xaxis.set_major_formatter(formatter)

    plt.xticks(rotation=45)
    plt.grid(True)
    plt.tight_layout()
    plt.show()


# In[28]:


data.columns


# In[29]:


# ç›¸å…³æ€§æ£€éªŒ
feature_cols = [col for col in data.columns if col not in ["date", 'ncd',"log_ncd", 'ncd_diff','CPI']]

# Pearson
pearson_corr = data[feature_cols].corr()["ncd_next_month"]

# Spearman
spearman_corr = data[feature_cols].corr(method="spearman")["ncd_next_month"]

print("Pearsonç›¸å…³ç³»æ•°ï¼š")
print(pearson_corr)

print("\nSpearmanç›¸å…³ç³»æ•°ï¼š")
print(spearman_corr)

print('*' * 100)

# é˜ˆå€¼
threshold = 0.01

# ç­›é€‰
selected_pearson = pearson_corr[pearson_corr.abs() > threshold].index.tolist()
selected_spearman = spearman_corr[spearman_corr.abs() > threshold].index.tolist()

# å»æ‰ncd_tomorrowæœ¬èº«
# selected_pearson = [f for f in selected_pearson if f != "ncd_next_week"]
# selected_spearman = [f for f in selected_spearman if f != "ncd_next_week"]

print("Pearsonç­›é€‰å‡ºçš„å› å­ï¼š", selected_pearson)
print("Spearmanç­›é€‰å‡ºçš„å› å­ï¼š", selected_spearman)
print('*' * 100)

# å–äº¤é›†
final_factors = list(set(selected_pearson) & set(selected_spearman))
print("æœ€ç»ˆç­›é€‰å› å­ï¼š", final_factors)


# In[30]:


# è®¡ç®—Spearman IC
from scipy.stats import spearmanr

print("æœªæ¥1æœˆICï¼š")
for f in feature_cols:
    valid_idx = data[[f, "ncd_next_month"]].dropna().index
    ic = spearmanr(
        data.loc[valid_idx, f],
        data.loc[valid_idx, "ncd_next_month"]
    ).correlation
    print(f"{f}: IC = {ic:.4f}")


# In[31]:


# æå– ICå€¼ç­›é€‰è¿‡çš„å› å­DataFrame
X_final = data[final_factors]

# è®¡ç®—ç›¸å…³çŸ©é˜µ
corr_matrix = X_final.corr()

# æ‰“å°
print(corr_matrix)


# In[32]:


# å­˜å‚¨é«˜ç›¸å…³å› å­å¯¹
high_corr_pairs = []

print("\nä»¥ä¸‹å› å­å¯¹ç›¸å…³ç³»æ•° > 0.8ï¼š")
for i in range(len(final_factors)):
    for j in range(i+1, len(final_factors)):
        f1 = final_factors[i]
        f2 = final_factors[j]
        corr_ij = corr_matrix.loc[f1, f2]
        if abs(corr_ij) > 0.8:
            print(f"  {f1} ä¸ {f2} çš„ç›¸å…³ç³»æ•° = {corr_ij:.4f}")
            high_corr_pairs.append((f1, f2, corr_ij))

high_corr_df = pd.DataFrame(high_corr_pairs, columns=["factor_1", "factor_2", "correlation"])


# In[33]:


# è¦å‰”é™¤çš„å˜é‡
target_col = ['ncd_next_month']
manual_remove = ['ncd1y','ncd3m','CPI_yoy','qtld_30','IBO007','R007','R001','IBO001']
# manual_remove = ['CPI_yoy', 'R001','IBO001','FR007','qtlu_30','reverse_repo_7d','DR007','DR001','IBO007']

# è¿‡æ»¤å˜é‡å
final_factors_no_collinearity = [col for col in final_factors if col not in manual_remove + target_col]

# æ˜¾ç¤ºç»“æœ
print("æœ€ç»ˆå› å­æ•°é‡ï¼š", len(final_factors_no_collinearity))
print("æœ€ç»ˆå› å­åˆ—è¡¨ï¼š", final_factors_no_collinearity)


# In[34]:


# å­˜å‚¨é«˜ç›¸å…³å› å­å¯¹
high_corr_pairs = []

print("\nä»¥ä¸‹å› å­å¯¹ç›¸å…³ç³»æ•° > 0.8ï¼š")
for i in range(len(final_factors_no_collinearity)):
    for j in range(i+1, len(final_factors_no_collinearity)):
        f1 = final_factors_no_collinearity[i]
        f2 = final_factors_no_collinearity[j]
        corr_ij = corr_matrix.loc[f1, f2]
        if abs(corr_ij) > 0.8:
            print(f"  {f1} ä¸ {f2} çš„ç›¸å…³ç³»æ•° = {corr_ij:.4f}")
            high_corr_pairs.append((f1, f2, corr_ij))

# å¯é€‰ï¼šå°†ç»“æœä¿å­˜ä¸º DataFrame ä»¥ä¾¿å¯¼å‡ºæˆ–æ’åº
high_corr_df = pd.DataFrame(high_corr_pairs, columns=["factor_1", "factor_2", "correlation"])


# In[56]:


# å·®å€¼
data["ncd_diff_month"] = data["ncd_next_month"] - data["ncd"]

# é˜ˆå€¼
epsilon = 0.005

# åˆ†ç±»æ ‡ç­¾
def classify(x):
    if x > epsilon:
        return 1
    elif x <= -epsilon:
        return 0

data["ncd_label"] = data["ncd_diff_month"].apply(classify)

# çœ‹åˆ†å¸ƒ
print(data["ncd_label"].value_counts())

# å»é™¤Naå€¼
# data = data.dropna().reset_index(drop=True)


# In[57]:


class Para:
    percent_cv = 0.2
    percent_train = 0.8
    seed = 42


# In[58]:


data.columns


# In[59]:


# xå’Œy
x = data[final_factors_no_collinearity]
y = data["ncd_label"]

# æŒ‰è¡Œæ•°åˆ‡åˆ†
train_size = int(len(data) * Para.percent_train)

x_train = x.iloc[:train_size]
x_test = x.iloc[train_size:]

y_train = y.iloc[:train_size]
y_test = y.iloc[train_size:]

# åˆå§‹åŒ–å˜æ¢å™¨ï¼ˆå¼ºåˆ¶æ­£æ€åˆ†å¸ƒï¼‰
# qt = QuantileTransformer(output_distribution='normal', random_state=42)

# æ‹Ÿåˆ y_train å¹¶å¯¹ y_train/y_test åšæ­£æ€æ˜ å°„
# y_train_normal = qt.fit_transform(y_train.values.reshape(-1, 1)).ravel()
# y_test_normal = qt.transform(y_test.values.reshape(-1, 1)).ravel()


# # Apply Genetic Programming
# function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log', 'abs', 'neg', 'inv']
# gp = SymbolicTransformer(
#     generations=10,               # è¿›åŒ–ä»£æ•° (å‡å°‘ä»¥ä¾¿æ¼”ç¤º)
#     population_size=1000,          # ç§ç¾¤å¤§å° (å‡å°‘ä»¥ä¾¿æ¼”ç¤º)
#     hall_of_fame=200,              # è£èª‰å ‚ï¼šä¿ç•™å¤šå°‘ä¸ªå†å²ä¸Šæœ€ä¼˜ç§€çš„å…¬å¼
#     n_components=10,               # æœ€ç»ˆå¸Œæœ›å¾—åˆ°å¤šå°‘ä¸ªæœ€ä¼˜çš„æ–°ç‰¹å¾
#     function_set=function_set,    # å…è®¸ä½¿ç”¨çš„æ•°å­¦å‡½æ•°
#     parsimony_coefficient=0.0005, # èŠ‚ä¿­ç³»æ•°ï¼šä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œé˜²æ­¢å…¬å¼å˜å¾—è¿‡äºå¤æ‚
#     max_samples=0.8,              # æ¯æ¬¡è¯„ä¼°æ—¶ä½¿ç”¨çš„æ ·æœ¬æ¯”ä¾‹ï¼Œå¢åŠ éšæœºæ€§
#     verbose=1,                    # æ‰“å°è¿›åŒ–è¿‡ç¨‹
#     random_state=42,
#     n_jobs=-1                     # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
# )
# 
# gp.fit(x_train, y_train)
# gp_features = gp.transform(x_train)
# gp_feature_names = [f'gp_feature_{i}' for i in range(gp_features.shape[1])]
# x_train_gp = pd.concat([x_train.reset_index(drop=True), pd.DataFrame(gp_features, columns=gp_feature_names)], axis=1)
# x_train_gp.head()

# # è¾“å‡ºå‰å‡ ä¸ª GP ç‰¹å¾çš„è¡¨è¾¾å¼ï¼ˆå‡½æ•°ç»„åˆæ–¹å¼ï¼‰
# print("GP ç‰¹å¾æ„é€ å‡½æ•°ï¼š")
# for i, program in enumerate(gp._best_programs):
#     print(f"gp_feature_{i}: {program}")

# # æå–è¡¨è¾¾å¼å­—ç¬¦ä¸²
# gp_expressions = [str(p) for p in gp._best_programs]
# 
# # å»é‡åä¿ç•™ç´¢å¼•ä½ç½®ï¼ˆç”¨äºä¿ç•™å¯¹åº”çš„åˆ—ï¼‰
# unique_expr, unique_indices = np.unique(gp_expressions, return_index=True)
# 
# # æ ¹æ®ç´¢å¼•ä¿ç•™å”¯ä¸€åˆ—
# gp_unique_features = gp_features[:, unique_indices]
# 
# # ç»™å®šæ–°çš„åˆ—å
# gp_unique_names = [f"gp_feature_{i}" for i in range(len(unique_indices))]
# 
# # æ„é€  DataFrameï¼Œå…ˆä¿ç•™è¡¨è¾¾å¼å”¯ä¸€åçš„ç‰¹å¾
# gp_df = pd.DataFrame(gp_unique_features, columns=gp_unique_names)
# 
# # æ›´æ–°ç‰¹å¾å
# gp_df.columns = [f"gp_feature_{i}" for i in range(gp_df.shape[1])]
# 
# # æ·»åŠ ç›®æ ‡åˆ—ï¼ˆlog_ncd_avg_next_monthï¼‰
# gp_df["target"] = y_train
# 
# # === è®¡ç®— Spearman IC å€¼ ===
# print("GP ç‰¹å¾ Spearman IC å€¼ï¼š")
# gp_ic_list = []
# for col in gp_df.columns[:-1]:  # æ’é™¤ target
#     ic = spearmanr(gp_df[col], gp_df["target"]).correlation
#     print(f"{col}: IC = {ic:.4f}")
#     gp_ic_list.append((col, ic))
# 
# # ICç­›é€‰
# ic_threshold = 0.03
# selected_gp_features = [name for name, ic in gp_ic_list if abs(ic) > ic_threshold]
# print("ICç­›é€‰åçš„GPç‰¹å¾ï¼š", selected_gp_features)
# 
# # æå–æœ€ç»ˆä¿ç•™çš„ç‰¹å¾
# gp_df_selected = gp_df[selected_gp_features]
# 
# # æ‹¼æ¥è®­ç»ƒé›†ï¼šä¿ç•™ç»è¿‡è¡¨è¾¾å¼ + æ•°å€¼å»é‡ + ICç­›é€‰çš„ç‰¹å¾
# x_train = pd.concat([x_train.reset_index(drop=True), gp_df_selected.reset_index(drop=True)], axis=1)
# 
# # ============ å¯¹ x_test åšç›¸åŒçš„ GP ç‰¹å¾å˜æ¢ ============
# 
# # 1. ç”¨å·²è®­ç»ƒå¥½çš„ GP å˜æ¢å™¨ç”Ÿæˆ x_test çš„ç‰¹å¾
# gp_features_test = gp.transform(x_test.reset_index(drop=True))
# 
# # 4. åˆ—åä¸è®­ç»ƒé›†ä¿æŒä¸€è‡´
# gp_df_test.columns = [f"gp_feature_{i}" for i in range(gp_df_test.shape[1])]
# 
# # 5. ä¿ç•™è®­ç»ƒé›†ä¸­ IC > é˜ˆå€¼çš„åˆ—ï¼ˆselected_gp_features æ˜¯è®­ç»ƒé›†é€‰å‡ºçš„ï¼‰
# gp_df_test_selected = gp_df_test[selected_gp_features]
# 
# # 6. æ‹¼æ¥åˆ°æµ‹è¯•é›†
# x_test = pd.concat([x_test.reset_index(drop=True), gp_df_test_selected.reset_index(drop=True)], axis=1)

# In[60]:


x_test.tail()


# for col in x_train.columns:
#     mean = x_train[col].mean()
#     std = x_train[col].std()
#     lower = mean - 2 * std
#     upper = mean + 2 * std
# 
#     x_train[col] = x_train[col].astype(float).clip(lower=lower, upper=upper)
#     x_test[col] = x_test[col].astype(float).clip(lower=lower, upper=upper)

# In[61]:


print(x_test.isna().sum().sort_values(ascending=False).head(10))


# In[62]:


# æ ¹æ®å› å­é‡è¦æ€§ç­›é€‰ç‰¹å¾
# è®­ç»ƒé»˜è®¤ XGBoost æ¨¡å‹ï¼ˆä¸è°ƒå‚ï¼‰
xgb_default = XGBRegressor(random_state=42)
xgb_default.fit(x_train, y_train)

# æå–ç‰¹å¾é‡è¦æ€§
importances = pd.Series(xgb_default.feature_importances_, index=x_train.columns)
importances = importances.sort_values(ascending=False)

# å¯è§†åŒ–å‰ 20 ä¸ªé‡è¦ç‰¹å¾
plt.figure(figsize=(8, 6))
importances.head(20).plot(kind='barh')
plt.title("Top 20 Feature Importances (Default XGBoost)")
plt.gca().invert_yaxis()
plt.grid(True)
plt.tight_layout()
plt.show()

# é€‰æ‹© Top N ç‰¹å¾ï¼ˆä½ å¯æ ¹æ®éœ€è¦è°ƒæ•´ Nï¼‰
top_n = 15
selected_features = importances.head(top_n).index.tolist()
print(f"Selected Top {top_n} features:", selected_features)

# æ›¿æ¢è®­ç»ƒ/æµ‹è¯•é›†ä¸ºç­›é€‰åçš„ç‰ˆæœ¬
x_train_selected = x_train[selected_features].copy()
x_test_selected = x_test[selected_features].copy()


# In[63]:


# è®¡ç®—ç±»åˆ«æ¯”ä¾‹ï¼ˆå¤šæ•°ç±»æ ·æœ¬æ•° / å°‘æ•°ç±»æ ·æœ¬æ•°ï¼‰
import numpy as np
from collections import Counter

counter = Counter(y_train)
print(counter)

# ä¸¾ä¾‹ï¼šè‹¥ç±»åˆ« 0 æœ‰ 560 ä¸ªï¼Œç±»åˆ« 1 æœ‰ 220 ä¸ªï¼š
# åˆ™ scale_pos_weight = 560 / 220 â‰ˆ 2.55
scale_pos_weight = counter[0] / counter[1]


# In[64]:


from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import classification_report, confusion_matrix

# 1. å®šä¹‰æ¨¡å‹
xgb_model = XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    random_state=Para.seed,
    scale_pos_weight=scale_pos_weight
)

# 2. å‚æ•°ç½‘æ ¼ï¼ˆå¯æŒ‰éœ€è°ƒæ•´ï¼‰
param_grid = {
    'n_estimators': [400, 800],
    'max_depth': [3, 6],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.5,0.75],
    'colsample_bytree': [0.4,0.6]
}

# 3. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
tscv = TimeSeriesSplit(n_splits=5)

# 4. ç½‘æ ¼æœç´¢
grid_search_xgb = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    cv=tscv,
    scoring='f1_macro',
    verbose=1,
    n_jobs=-1
)

# 5. æ¨¡å‹æ‹Ÿåˆ
grid_search_xgb.fit(x_train, y_train)

# 6. è¾“å‡ºæœ€ä¼˜å‚æ•°ä¸åˆ†æ•°
print("Best parameters:", grid_search_xgb.best_params_)
print("Best CV F1 score:", grid_search_xgb.best_score_)

# 7. æµ‹è¯•é›†é¢„æµ‹ä¸è¯„ä¼°
y_pred_xgb = grid_search_xgb.predict(x_test)

print("\n=== XGBoost æµ‹è¯•é›†æ€§èƒ½ ===")
print(classification_report(y_test, y_pred_xgb))
print(confusion_matrix(y_test, y_pred_xgb))


# In[65]:


from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# è·å–é¢„æµ‹æ¦‚ç‡ï¼ˆæ³¨æ„æ˜¯ predict_probaï¼‰
y_score = grid_search_xgb.best_estimator_.predict_proba(x_test)[:, 1]

# è®¡ç®— ROC æ›²çº¿å’Œ AUC
fpr, tpr, thresholds = roc_curve(y_test, y_score)
roc_auc = auc(fpr, tpr)

# ç»˜å›¾
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'XGBoost AUC = {roc_auc:.4f}')
plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve on Test Set')
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.show()


# In[66]:


# æŸ¥çœ‹è®­ç»ƒé›†å„ç±»æ•°é‡
print("è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒï¼š")
print(y_train.value_counts())

# æŸ¥çœ‹æµ‹è¯•é›†å„ç±»æ•°é‡
print("æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒï¼š")
print(y_test.value_counts())


# In[67]:


x_test_new = x_test.copy()  # æ˜¾å¼åˆ›å»ºå‰¯æœ¬ï¼Œé¿å… SettingWithCopyWarning
# ä» best_estimator_ è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆä½¿ç”¨ gainï¼‰
booster = grid_search_xgb.best_estimator_.get_booster()
importance_dict = booster.get_score(importance_type='gain')
importance_series = pd.Series(importance_dict)
importance_series = importance_series / importance_series.sum()

# åªä¿ç•™æ¨¡å‹ä¸­å®é™…ä½¿ç”¨çš„ç‰¹å¾
valid_cols = [col for col in x_test.columns if col in importance_series.index]

# æ„é€ ç»¼åˆå› å­
x_test_new["ncd_combined_factor"] = x_test_new[valid_cols].mul(importance_series[valid_cols], axis=1).sum(axis=1)

# å‡è®¾ data æ˜¯åŸå§‹å®Œæ•´çš„ DataFrameï¼ŒåŒ…å«æ‰€æœ‰å˜é‡ï¼ŒåŒ…æ‹¬ ncdã€dateã€ncd_next_month ç­‰
x_test_new["ncd"] = data.loc[x_test_new.index, "ncd"].values
x_test_new["date"] = data.loc[x_test_new.index, "date"].values
x_test_new["ncd_next_month"] = data.loc[x_test_new.index, "ncd_next_month"].values

# æŸ¥çœ‹å‰å‡ è¡Œ
print(x_test_new[["ncd_combined_factor"]].head())


# In[68]:


# æ„é€ æ ‡å‡†åŒ–åçš„ signal
mean = x_test_new["ncd_combined_factor"].mean()
std = x_test_new["ncd_combined_factor"].std()
standardized = (x_test_new["ncd_combined_factor"] - mean) / std

# è½¬ä¸ºæ¦‚ç‡ signal
from scipy.special import expit
x_test_new["ncd_combined_signal"] = expit(2.5 * standardized)


# In[69]:


x_test_new.head(50)


# In[70]:


# è®¡ç®—æ»‘åŠ¨å¹³å‡
x_test_new["signal_smooth"] = x_test_new["ncd_combined_signal"].rolling(5).mean()

# ç„¶åç»˜å›¾
plt.figure(figsize=(12, 5))
plt.plot(x_test_new['date'], x_test_new['signal_smooth'], label='5D Smoothed Signal', color='blue')
plt.axhline(0.6, color='red', linestyle='--', label='Bullish Threshold (0.6)')
plt.axhline(0.4, color='green', linestyle='--', label='Bearish Threshold (0.4)')

plt.title("Smoothed Model Signal (5D MA of ncd_combined_signal)")
plt.xlabel("Date")
plt.ylabel("Signal")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


# In[71]:


# å‡è®¾ä½ å·²æœ‰åˆ—ï¼šsignal_smoothã€ncdã€ncd_next_month

def classify_signal(s):
    if s >= 0.6:
        return 1  # çœ‹å¤š
    elif s < 0.4:
        return 0  # çœ‹ç©º
    else:
        return np.nan  # ä¸­æ€§ï¼Œä¸å‡ºä¿¡å·

x_test_new["signal_label"] = x_test_new["signal_smooth"].apply(classify_signal)


# In[72]:


# å‰å‘å¡«å……ä¿¡å· â†’ è¿ç»­è¯†åˆ«
x_test_new["regime"] = x_test_new["signal_label"].ffill()
x_test_new["regime_change"] = x_test_new["regime"] != x_test_new["regime"].shift(1)

# ç»™æ¯æ®µè¿ç»­åŒºé—´ç¼–å·
x_test_new["regime_id"] = x_test_new["regime_change"].cumsum()

# å»æ‰ä¸­æ€§åŒºé—´ï¼ˆregimeä¸ºNaNï¼‰
signal_df = x_test_new.dropna(subset=["regime"]).copy()


# In[73]:


intervals = []

# éå†æ¯ä¸ª regime åŒºé—´
for regime_id, group in signal_df.groupby("regime_id"):
    start_date = group["date"].iloc[0]
    end_date = group["date"].iloc[-1]
    regime_type = "çœ‹å¤š" if group["regime"].iloc[0] == 1 else "çœ‹ç©º"
    n_days = len(group)
    
    # åŒºé—´é¦–å°¾çš„ ncd å€¼
    ncd_start = group["ncd"].iloc[0]
    ncd_end = group["ncd"].iloc[-1]
    change_bp = (ncd_end - ncd_start) * 100  # å•ä½ bp
    max_drawdown = (group["ncd"] - group["ncd"].cummax()).min() * 100  # bp
    
    # èƒœè´Ÿåˆ¤æ–­ï¼ˆçœ‹å¤š â†’ æ”¶ç›Šç‡ä¸‹é™ä¸ºæ­£ç¡®ï¼›çœ‹ç©º â†’ æ”¶ç›Šç‡ä¸Šå‡ä¸ºæ­£ç¡®ï¼‰
    is_correct = (change_bp < 0) if regime_type == "çœ‹å¤š" else (change_bp > 0)

    intervals.append({
        "èµ·å§‹æ—¥æœŸ": start_date,
        "ç»“æŸæ—¥æœŸ": end_date,
        "ä¿¡å·ç±»å‹": regime_type,
        "æŒç»­å¤©æ•°": n_days,
        "åŒºé—´å˜åŠ¨(bp)": round(change_bp, 2),
        "æœ€å¤§å›æ’¤(bp)": round(max_drawdown, 2),
        "æ–¹å‘æ˜¯å¦æ­£ç¡®": "æ­£ç¡®" if is_correct else "é”™è¯¯"
    })

# è½¬ä¸ºè¡¨æ ¼
interval_df = pd.DataFrame(intervals)

# æ˜¾ç¤ºè¡¨æ ¼
import pandas as pd
from IPython.display import display
display(interval_df)


# In[74]:


# æ€»åŒºé—´æ•°
total_intervals = len(interval_df)

# æ­£ç¡®æ–¹å‘æ•°é‡
num_correct = (interval_df["æ–¹å‘æ˜¯å¦æ­£ç¡®"] == "æ­£ç¡®").sum()

# èƒœç‡
win_rate = num_correct / total_intervals

# å¹³å‡åŒºé—´æ”¶ç›Šï¼ˆbpï¼‰
avg_return = interval_df["åŒºé—´å˜åŠ¨(bp)"].mean()

# å¹³å‡æœ€å¤§å›æ’¤ï¼ˆbpï¼‰
avg_drawdown = interval_df["æœ€å¤§å›æ’¤(bp)"].mean()

# è¾“å‡º
print(f"ğŸ“ˆ æ€»åŒºé—´æ•°: {total_intervals}")
print(f"âœ… æ­£ç¡®æ–¹å‘æ•°: {num_correct}")
print(f"ğŸ† èƒœç‡: {win_rate:.2%}")
print(f"ğŸ“Š å¹³å‡åŒºé—´å˜åŠ¨: {avg_return:.2f} bp")
print(f"ğŸ“‰ å¹³å‡æœ€å¤§å›æ’¤: {avg_drawdown:.2f} bp")


# In[ ]:




